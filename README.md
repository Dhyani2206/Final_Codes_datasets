- **Definition:** One major challenge is the inefficiency of the current website search process. In order to guarantee precise and thorough results, the goal is to optimise and improve the search functionality. To improve user experience, pertinent keywords must be seamlessly associated with particular topics. To solve these issues and boost search efficiency, provided metadata must be mapped in order to correlate relevant keywords with website content. 
- **Models Compared:** BERT and Sentence Transformer
- **About BERT:**
  1. BERT is a transformer-based model that learns bidirectional representations of text. It consists of multiple transformer encoder layers.
  2. During fine-tuning for dense retrieval, BERT takes the input text (queries or documents) and produces a fixed-size embedding vector for each input sequence.
- **About Sentence Transformer:**
  1. These architectures learn to project similar sentences close to each other in the embedding space while pushing dissimilar sentences apart.
  2. Various pretraining tasks can be used, such as pairwise sentence similarity prediction or triplet loss.
  3. Sentence transformers typically use architectures like BERT or RoBERTa as the base model but adapt them for sentence-level embeddings.
- **Sentence Transformer vs BERT:**
![image](https://github.com/user-attachments/assets/6004c7b1-4f26-4d65-bae8-5a2b6b746368)




